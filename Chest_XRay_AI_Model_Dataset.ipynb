{ "cells": [ { "cell_type": "markdown", "metadata": {}, "source": [ "# Chest X-Ray AI Model: Dataset Download and Preparation\n", "\n", "## Milestone 5: Downloading and Preparing Datasets\n", "\n", "This notebook guides you through downloading chest X-ray datasets and preparing them for the AI model to classify images into fracture, pneumonia, tuberculosis, tumor, or normal. The process is designed for a laptop with 16GB RAM, Ryzen 7 CPU, and 4GB AMD Radeon RX GPU, using Python and Jupyter Notebook.\n", "\n", "Prerequisites:\n", "- Completed Milestone 4 (environment setup).\n", "- Kaggle API configured with kaggle.json in the correct location.\n", "- Project directory created (./chest_xray_ai_project/).\n", "\n", "Datasets:\n", "- NIH Chest X-ray Dataset: Contains pneumonia and normal images.\n", "- TBX11K: Contains tuberculosis images.\n", "- Fracture/Tumor Datasets: Sourced from Kaggle (e.g., Bone Fracture Dataset, Chest Tumor Dataset).\n", "\n", "Note: Run cells sequentially. Ensure ~50GB free disk space for datasets. If errors occur, check the troubleshooting tips.\n", "\n", "## Step 1: Import Libraries\n", "- Goal: Load necessary Python libraries for dataset handling.\n", "- Libraries:\n", " - kaggle: For downloading datasets.\n", " - pandas: For label management.\n", " - os, shutil: For file operations.\n", "- Expected Output: No errors." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "import kaggle\n", "import pandas as pd\n", "import os\n", "import shutil\n", "\n", "print('Libraries imported successfully')" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Step 2: Set Up Directory Structure\n", "- Goal: Create directories for raw and processed datasets.\n", "- Instructions:\n", " 1. Use the project directory from Milestone 4.\n", " 2. Create subdirectories for each dataset and splits (train/val/test).\n", "- Expected Output: Directory structure created." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "project_dir = './chest_xray_ai_project'\n", "dataset_dir = f'{project_dir}/datasets'\n", "\n", "# Create directories for raw datasets\n", "os.makedirs(f'{dataset_dir}/nih', exist_ok=True)\n", "os.makedirs(f'{dataset_dir}/tbx11k', exist_ok=True)\n", "os.makedirs(f'{dataset_dir}/fracture', exist_ok=True)\n", "os.makedirs(f'{dataset_dir}/tumor', exist_ok=True)\n", "\n", "# Create directories for processed data (train/val/test)\n", "os.makedirs(f'{dataset_dir}/processed/train', exist_ok=True)\n", "os.makedirs(f'{dataset_dir}/processed/val', exist_ok=True)\n", "os.makedirs(f'{dataset_dir}/processed/test', exist_ok=True)\n", "\n", "print(f'Dataset directories created at: {dataset_dir}')" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Step 3: Download Datasets\n", "- Goal: Download chest X-ray datasets using Kaggle API.\n", "- Datasets:\n", " - NIH Chest X-ray: nih-chest-xrays/data (large, ~40GB).\n", " - TBX11K: raddar/tbx11k.\n", " - Fracture: vuppalaadithyasai/bone-fracture-detection (example; replace if needed).\n", " - Tumor: Placeholder (search Kaggle for 'chest tumor X-ray' or use subset of NIH).\n", "- Instructions:\n", " 1. Run the code below to download datasets.\n", " 2. Ensure a stable internet connection; NIH dataset is large.\n", " 3. For tumors, we’ll use a placeholder and update later if a specific dataset is found.\n", "- Note: If a dataset fails to download, verify Kaggle API credentials or try manually downloading from Kaggle.\n", "- Expected Output: Datasets downloaded and unzipped." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Download NIH Chest X-ray dataset\n", "kaggle.api.dataset_download_files('nih-chest-xrays/data', path=f'{dataset_dir}/nih', unzip=True)\n", "\n", "# Download TBX11K dataset\n", "kaggle.api.dataset_download_files('raddar/tbx11k', path=f'{dataset_dir}/tbx11k', unzip=True)\n", "\n", "# Download Fracture dataset (example)\n", "kaggle.api.dataset_download_files('vuppalaadithyasai/bone-fracture-detection', path=f'{dataset_dir}/fracture', unzip=True)\n", "\n", "# Placeholder for Tumor dataset (update with actual dataset if found)\n", "print('Tumor dataset: Please search Kaggle for "chest tumor X-ray" and manually download to {dataset_dir}/tumor')\n", "\n", "print('Datasets downloaded successfully')" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Step 4: Load and Map Labels\n", "- Goal: Load dataset labels and map them to 5 classes (normal, fracture, pneumonia, tuberculosis, tumor).\n", "- Instructions:\n", " 1. Load metadata (e.g., CSV files) for each dataset.\n", " 2. Create a unified DataFrame with columns: image_path, label.\n", " 3. Map labels to integers: {0: normal, 1: fracture, 2: pneumonia, 3: tuberculosis, 4: tumor}.\n", "- Note: Dataset-specific label formats vary. We’ll handle NIH and TBX11K explicitly; fracture/tumor datasets may need manual adjustment.\n", "- Expected Output: Unified DataFrame with image paths and labels." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "# Initialize empty DataFrame for unified labels\n", "data = pd.DataFrame(columns=['image_path', 'label'])\n", "\n", "# NIH Chest X-ray: Load labels (example assumes 'Data_Entry_2017.csv')\n", "nih_csv = f'{dataset_dir}/nih/Data_Entry_2017.csv'\n", "if os.path.exists(nih_csv):\n", " nih_data = pd.read_csv(nih_csv)\n", " # Map findings to classes (simplified; assumes 'No Finding' = normal, 'Pneumonia' = pneumonia)\n", " nih_data['label'] = nih_data['Finding Labels'].apply(\n", " lambda x: 2 if 'Pneumonia' in x else 0 if 'No Finding' in x else -1\n", " )\n", " nih_data = nih_data[nih_data['label'] != -1][['Image Index', 'label']]\n", " nih_data['image_path'] = nih_data['Image Index'].apply(lambda x: f'{dataset_dir}/nih/images/{x}')\n", " data = pd.concat([data, nih_data[['image_path', 'label']]], ignore_index=True)\n", "\n", "# TBX11K: Load labels (assumes 'annotations.csv' or similar)\n", "tbx_csv = f'{dataset_dir}/tbx11k/annotations.csv' # Update path if different\n", "if os.path.exists(tbx_csv):\n", " tbx_data = pd.read_csv(tbx_csv)\n", " tbx_data['label'] = 3 # Tuberculosis\n", " tbx_data['image_path'] = tbx_data['image_id'].apply(lambda x: f'{dataset_dir}/tbx11k/images/{x}')\n", " data = pd.concat([data, tbx_data[['image_path', 'label']]], ignore_index=True)\n", "\n", "# Fracture: Load labels (assumes folder structure or CSV)\n", "fracture_dir = f'{dataset_dir}/fracture'\n", "if os.path.exists(fracture_dir):\n", " # Example: Assume images in subfolders 'fracture' and 'normal'\n", " fracture_images = []\n", " for label, folder in [(1, 'fracture'), (0, 'normal')]:\n", " folder_path = f'{fracture_dir}/{folder}'\n", " if os.path.exists(folder_path):\n", " images = [f'{folder_path}/{img}' for img in os.listdir(folder_path) if img.endswith(('.png', '.jpg'))]\n", " fracture_images.extend([(img, label) for img in images])\n", " fracture_data = pd.DataFrame(fracture_images, columns=['image_path', 'label'])\n", " data = pd.concat([data, fracture_data], ignore_index=True)\n", "\n", "# Tumor: Placeholder (update with actual dataset)\n", "print('Tumor dataset labels: Manually add labels when dataset is available')\n", "\n", "# Save unified DataFrame\n", "data.to_csv(f'{dataset_dir}/processed/labels.csv', index=False)\n", "\n", "print('Unified labels saved to labels.csv')\n", "print(data['label'].value_counts())" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Step 5: Split Data into Train/Validation/Test\n", "- Goal: Split dataset into 70% train, 15% validation, 15% test.\n", "- Instructions:\n", " 1. Use sklearn to perform stratified splitting.\n", " 2. Save splits as separate CSV files.\n", "- Expected Output: Three CSV files (train.csv, val.csv, test.csv)." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "from sklearn.model_selection import train_test_split\n", "\n", "# Ensure labels are integers\n", "data['label'] = data['label'].astype(int)\n", "\n", "# Split: 70% train, 30% temp (to split into val/test)\n", "train_data, temp_data = train_test_split(data, test_size=0.3, stratify=data['label'], random_state=42)\n", "\n", "# Split temp: 50% val, 50% test (15% each of total)\n", "val_data, test_data = train_test_split(temp_data, test_size=0.5, stratify=temp_data['label'], random_state=42)\n", "\n", "# Save splits\n", "train_data.to_csv(f'{dataset_dir}/processed/train.csv', index=False)\n", "val_data.to_csv(f'{dataset_dir}/processed/val.csv', index=False)\n", "test_data.to_csv(f'{dataset_dir}/processed/test.csv', index=False)\n", "\n", "print('Data splits saved:')\n", "print(f'Train: {len(train_data)} images')\n", "print(f'Validation: {len(val_data)} images')\n", "print(f'Test: {len(test_data)} images')" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Step 6: Verify Dataset\n", "- Goal: Check that images and labels are accessible.\n", "- Instructions:\n", " 1. Load a few images to ensure paths are correct.\n", " 2. Display sample images and labels.\n", "- Expected Output: Sample images displayed with labels." ] }, { "cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": [ "import cv2\n", "import matplotlib.pyplot as plt\n", "\n", "# Load train.csv\n", "train_data = pd.read_csv(f'{dataset_dir}/processed/train.csv')\n", "\n", "# Display first 3 images\n", "label_map = {0: 'Normal', 1: 'Fracture', 2: 'Pneumonia', 3: 'Tuberculosis', 4: 'Tumor'}\n", "plt.figure(figsize=(15, 5))\n", "for i in range(3):\n", " if i < len(train_data):\n", " img_path = train_data.iloc[i]['image_path']\n", " label = train_data.iloc[i]['label']\n", " img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n", " if img is not None:\n", " plt.subplot(1, 3, i+1)\n", " plt.imshow(img, cmap='gray')\n", " plt.title(f'Label: {label_map[label]}')\n", " plt.axis('off')\n", "plt.show()" ] }, { "cell_type": "markdown", "metadata": {}, "source": [ "## Troubleshooting Tips\n", "- Download fails: Check Kaggle API credentials or internet connection. Manually download from Kaggle if needed.\n", "- File not found: Verify dataset paths (e.g., nih/images/). Datasets may unzip into subfolders.\n", "- Label mapping issues: Check dataset CSV files for correct column names. Adjust code if metadata differs.\n", "- Memory errors: If loading large CSVs fails, process in chunks using pandas.read_csv(chunksize=1000).\n", "- Tumor dataset: Search Kaggle for 'chest tumor X-ray dataset' and manually integrate if found.\n", "\n", "## Next Steps\n", "- Save this notebook in ./chest_xray_ai_project/.\n", "- Run all cells to download and prepare datasets.\n", "- Verify images and labels display correctly.\n", "- Proceed to Milestone 6: Implement image preprocessing.\n", "\n", "Documentation Links:\n", "- Kaggle API: https://github.com/Kaggle/kaggle-api\n", "- Pandas: https://pandas.pydata.org/docs/\n", "- OpenCV: https://docs.opencv.org/" ] } ], "metadata": { "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" }, "language_info": { "codemirror_mode": { "name": "ipython", "version": 3 }, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.0" } }, "nbformat": 4, "nbformat_minor": 4 }